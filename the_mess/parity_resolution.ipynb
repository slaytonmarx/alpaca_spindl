{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activating with Paper\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys; sys.path.append('../lib/*/*'); sys.path.append('..')\n",
    "import alpaca_trade_api as alp\n",
    "import matplotlib.pyplot as plt\n",
    "import lib.TimeKeeper as tk\n",
    "import lib.Toolbox as tb\n",
    "import glob\n",
    "import lib.Broker as br\n",
    "import pandas as pd\n",
    "from lib.Config import Config\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as p\n",
    "import pytz\n",
    "from scipy import stats\n",
    "\n",
    "api = br.paper_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "During training we make predictions one when to buy and sell which are broadly mirrored, but not always, and further, when they are mirrored there is variance in whether the trade actually goes through. That is to say, we are recording it going through in training, but then it does not in trading. How do we reconcile these distinct discrepencies? Let us speak them more plainly:\n",
    "1. *Incomplete Parity*: Parity is not perfect, we must create perfect parity in all operational markers.\n",
    "2. *Trade Completion Parity*: Once we have perfect parity of information, we must still achieve perfect parity of outcome. We must ensure that we can measure when a trade will actually have gone through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incomplete Parity\n",
    "Ultimately we need to produce better records of what is actually being records and ensure extremely precise timekeeping. Honestly, since we can make so many querries, it may make sense to do a complete overhaul of our record collection program and instead literally pull each period as though we were going through it as we have... although, that's also a crazy thing to do...\n",
    "\n",
    "We just need to show that, 100% of the time, we have the exact same trades pulled AND we are able to get the exact same *history* as we have in training. After all, all of our metrics rely on the past 20 to 40 ticks.\n",
    "\n",
    "Ok, so we need to form up our trade pulling. Do we go so far as to keep records of ALL trades ever done so as to more easily pull them? Possibly, though I think it may lead to some severe issues in terms of git data bloat, but then we may be able to store them effectively so I'm not sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific next steps:\n",
    "1. Pull all trades for NVDA from the past like, three months and store them in pickle form.\n",
    "2. Make a function to check whether the history and the current data are the same.\n",
    "3. Make logs keep track of all of the data we're taking in, but in a compressed and simplified manner (basically, we feed a set of dataframes to it and it parses them into a single string, which we store under a column called \"bulk data\" or some such)\n",
    "4. Run an experiment on the current data being pulled by the robotrader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activating with Paper\n"
     ]
    }
   ],
   "source": [
    "# Pull trade info for archiving\n",
    "api = br.paper_api()\n",
    "\n",
    "trades = api.get_trades('NVDA', start=tk.to_time_s(2,1), feed='sip').df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activating with Paper\n",
      "['2024-02-15 00:00:00-04:00.pkl', '2024-02-08 00:00:00-04:00.pkl', '2024-02-01 00:00:00-04:00.pkl', '2024-02-06 00:00:00-04:00.pkl', '2024-02-12 00:00:00-04:00.pkl', '2024-02-16 00:00:00-04:00.pkl', '2024-02-13 00:00:00-04:00.pkl', '2024-02-14 00:00:00-04:00.pkl', '2024-02-07 00:00:00-04:00.pkl', '2024-02-09 00:00:00-04:00.pkl', '2024-02-02 00:00:00-04:00.pkl', '2024-02-05 00:00:00-04:00.pkl']\n",
      "2024-02-19 00:00:00-04:00\n",
      "2024-03-29 00:00:00-04:00\n"
     ]
    }
   ],
   "source": [
    "api = br.paper_api()\n",
    "entries = [base.split('/')[-1] for base in glob.glob('../archive_data/raw_trades/NVDA/*')]\n",
    "print(entries)\n",
    "for date in (pd.date_range(tk.to_time(2,1),tk.to_time(4,24), freq='B')):\n",
    "    if str(date)+'.pkl' in entries: continue\n",
    "    trades = api.get_trades('NVDA', start=tk.dto_time(tk.get_market_open(date)), end=tk.dto_time(tk.get_market_close(date))).df\n",
    "    if len(trades) == 0: print(date); continue\n",
    "    trades[['price','size']].to_pickle('../archive_data/raw_trades/NVDA/'+str(date)+'.pkl')\n",
    "    #print(pd.read_pickle('../archive_data/raw_trades/NVDA/'+str(date)+'.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-04-24 19:00:00.010579607+00:00</th>\n",
       "      <td>792.3650</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-24 19:00:00.071332957+00:00</th>\n",
       "      <td>792.4818</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-24 19:00:00.153252316+00:00</th>\n",
       "      <td>792.3200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-24 19:00:00.185888115+00:00</th>\n",
       "      <td>792.2300</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-24 19:00:00.185890631+00:00</th>\n",
       "      <td>792.2300</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-24 19:47:09.508482317+00:00</th>\n",
       "      <td>795.8750</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-24 19:47:09.567990839+00:00</th>\n",
       "      <td>795.8440</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-24 19:47:09.607895009+00:00</th>\n",
       "      <td>795.9900</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-24 19:47:09.613854625+00:00</th>\n",
       "      <td>795.8750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-24 19:47:09.693545046+00:00</th>\n",
       "      <td>795.8750</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95137 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        price  size\n",
       "timestamp                                          \n",
       "2024-04-24 19:00:00.010579607+00:00  792.3650     1\n",
       "2024-04-24 19:00:00.071332957+00:00  792.4818     1\n",
       "2024-04-24 19:00:00.153252316+00:00  792.3200    10\n",
       "2024-04-24 19:00:00.185888115+00:00  792.2300     8\n",
       "2024-04-24 19:00:00.185890631+00:00  792.2300     5\n",
       "...                                       ...   ...\n",
       "2024-04-24 19:47:09.508482317+00:00  795.8750    85\n",
       "2024-04-24 19:47:09.567990839+00:00  795.8440   300\n",
       "2024-04-24 19:47:09.607895009+00:00  795.9900     5\n",
       "2024-04-24 19:47:09.613854625+00:00  795.8750     1\n",
       "2024-04-24 19:47:09.693545046+00:00  795.8750   100\n",
       "\n",
       "[95137 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Next Steps\n",
    "Ok, so, what are our next steps? We need to make a trade gathering archive (basically, what we need to do is store both the trades and the data, and be able to update them both with each tick). We need to create a function that can compare the trades historically with those that we collect in the present, and specifically look for discontenuity. So what do we do?\n",
    "1. We update get_data to both collect and store trade information and data information\n",
    "2. We update get_segment to both collect and store trade information and data information\n",
    "3. We update strategy get data to both collect and store trade information and data information\n",
    "4. We create a function which checks whether the collected data is the same as what we've got in the archives (and indeed, whether the archive is accurate/keeps continuity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would this change even look like? Well if we're keeping track of trades we need to determine how. I think we'll have each day of trades as it's own pickle file. Honestly, there's so much data with them it would be silly to splice them together. Speaking of splicing, we need a general purpose function for splicing two sets together, specifically only adding the relevant parts and hopefully not having to sort everything every time. To that end, we should also reduce the general amount of data being loaded at any given time, so if we do have to sort everytime we can do so intelligently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are we invisioning? get_segment fires every 10 seconds (or whatever the tick duration is) and then every minute we update the archive with what we've gathered. This will only work after we're certain that parity is perfect between our backtesting and our current."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strat_get_data(self, update:bool = False):\n",
    "    '''Pulls the given day's data. If update then we only pull a single minute and update our existing data'''\n",
    "    if update and hasattr(self, 'data') and tk.in_tick(self.data.index[-1]):\n",
    "        bar = tb.get_segment(self.symbol, self.api)\n",
    "        self.data.loc[bar.name] = bar\n",
    "    else:\n",
    "        s, e = tk.get_market_open(self.date), tk.get_market_close(self.date)\n",
    "        tb.update_archive(self.symbol)\n",
    "        self.data = tb.get_archive(self.symbol)[s:e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(symbol:str, start:pd.DatetimeIndex = tk.dto_time(tk.get_market_open()), end:pd.DatetimeIndex = tk.dto_time(tk.now()), api:alp.REST = None):\n",
    "    '''Returns the data of the given time'''\n",
    "    if not api: api = br.paper_api()\n",
    "\n",
    "    s, e = start if isinstance(start, str) else tk.dto_time(start), end if isinstance(end, str) else tk.dto_time(end)\n",
    "    df = api.get_trades(symbol, start=s, end=e, feed='sip').df\n",
    "    \n",
    "    if len(df) == 0: return None\n",
    "    else: df = df.price\n",
    "\n",
    "    df.index = df.index\n",
    "    ts = pd.Series(pd.date_range(df.index[0], end=df.index[-1], freq=str(tk.TICK_DURATION)+\"s\")).round('s')\n",
    "    data = []\n",
    "    for i in range(len(ts))[:-1]:\n",
    "        try:\n",
    "            f = df[ts.loc[i]:ts.loc[i+1]]\n",
    "            #print(f.index.min(),f.index.max(), len(f), print(ts.loc[i+1]))\n",
    "            f = f[(np.abs(stats.zscore(f)) < 1)]\n",
    "            if len(f) > 0: data.append({'date':ts.loc[i+1], 'open': f.iloc[0], 'close': f.iloc[-1], 'high':f.max(), 'low':f.min(), 'volume':len(f)})\n",
    "            else: data.append({'date':ts.loc[i+1], 'open': data[-1]['open'], 'close': data[-1]['close'], 'high':data[-1]['high'], 'low':data[-1]['low'], 'volume':len(f)})\n",
    "        except Exception as e: print(e)\n",
    "    if len(data) > 0:\n",
    "        data = pd.DataFrame(data).set_index('date')\n",
    "        data.index = data.index.tz_convert(pytz.timezone('America/New_York'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we Really Need\n",
    "What we really need is a function called \"get_data_up_to_now\" that does exactly what it says, getting the data from the beginning of the day up to this minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping for 7\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_data() missing 1 required positional argument: 'symbol'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tk\u001b[38;5;241m.\u001b[39msync()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_data() missing 1 required positional argument: 'symbol'"
     ]
    }
   ],
   "source": [
    "tk.sync()\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How will this actually work. Make a function that updates the trade pickle for a certain day (or creates it if non exist).\n",
    "'''\n",
    "    calls data_up_to_now\n",
    "        check if there's a data archive of today\n",
    "            if yes, grab it\n",
    "            if no:\n",
    "                check if there is a trades pickle for today, if not, create one up to now\n",
    "        if there is, pull it, if the time is up to now (within the tick amount) then bam, that's your \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.Broker as br\n",
    "import lib.TimeKeeper as tk\n",
    "import lib.Toolbox as tb\n",
    "import alpaca_trade_api as alp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import pytz\n",
    "\n",
    "def process_trades(df:pd.DataFrame):\n",
    "    '''Processes the trades given up to the most recent Timekeeper tick'''\n",
    "    ts, data = pd.Series(pd.date_range(df.index[0], end=df.index[-1], freq=str(tk.TICK_DURATION)+\"s\").round('s')), []\n",
    "    for i in range(1, len(ts)):\n",
    "        f = df[ts.loc[i-1]:ts.loc[i]].price\n",
    "        f = f[(np.abs(stats.zscore(f)) < 1)]\n",
    "        data.append({'date':ts.loc[i], 'open': f.iloc[0], 'close': f.iloc[-1], 'high':f.max(), 'low':f.min(), 'volume':len(f)})\n",
    "    if len(data) > 0:\n",
    "        data = pd.DataFrame(data).set_index('date')\n",
    "        data.index = data.index.tz_convert(pytz.timezone('America/New_York'))\n",
    "    return data\n",
    "\n",
    "def update_archive(symbol:str, date:pd.DatetimeIndex = tk.today()):\n",
    "    '''Collects the data of a ticker and adds it to a pre-existing archive, sorting and making sure there are no duplicates'''\n",
    "    trades = update_trades(symbol)\n",
    "    print(trades.index[-1] + pd.DateOffset(seconds=tk.TICK_DURATION))\n",
    "\n",
    "    datename = date.strftime(\"%Y-%m-%d\"); ppath = f'../archive_data/processed/{symbol}'; rpath = f'../archive_data/raw/{symbol}'\n",
    "    if not p.isfile(f'{ppath}/{datename}.pkl'): process_trades(trades).to_pickle(f'{ppath}/{datename}.pkl')\n",
    "    data = pd.read_pickle(f'{ppath}/{datename}.pkl')\n",
    "    if tk.get_market_open() > tk.now() or (data.index[-1] + pd.DateOffset(seconds=tk.TICK_DURATION) > tk.now()): return\n",
    "    data = easy_concat(data, process_trades(trades))\n",
    "    data.to_pickle(f'{ppath}/{datename}.pkl')\n",
    "    return data\n",
    "\n",
    "def update_trades(symbol:str, date:pd.DatetimeIndex = tk.today()):\n",
    "    '''Updates the trade info of the given day for the given symbol'''\n",
    "    datename = date.strftime(\"%Y-%m-%d\"); rpath = f'../archive_data/raw/{symbol}'\n",
    "    if not p.isfile(f'{rpath}/{datename}.pkl'): get_days_trades(symbol).to_pickle(f'{rpath}/{datename}.pkl')\n",
    "    trades = pd.read_pickle(f'{rpath}/{datename}.pkl')\n",
    "    trades = easy_concat(trades, get_trades(symbol, trades.index[-1], tk.get_market_close(date)))\n",
    "    trades.to_pickle(f'{rpath}/{datename}.pkl')\n",
    "    return trades\n",
    "\n",
    "def get_trades(symbol:str, s:pd.DatetimeIndex, e:pd.DatetimeIndex):\n",
    "    '''Functions as api.get_trades() except converts to a df with price and size and can handle datetime objects'''\n",
    "    if not isinstance(s, str): s = tk.dto_time(s)\n",
    "    if not isinstance(e, str): e = tk.dto_time(e)\n",
    "    trades = api.get_trades(symbol, start=s, end=e).df\n",
    "    if len(trades) == 0: return None\n",
    "    trades.index = trades.index.tz_convert(pytz.timezone('America/New_York'))\n",
    "    return  trades[['price','size']]\n",
    "\n",
    "def get_days_trades(symbol:str, date:pd.DatetimeIndex = tk.today()):\n",
    "    '''Returns the trades of just today'''\n",
    "    return get_trades(symbol, tk.get_market_open(date), tk.get_market_close(date))\n",
    "\n",
    "def get_week(symbol:str, date:pd.DatetimeIndex = tk.today(), api = None):\n",
    "    '''Returns the data of the last week as a dataframe (ignoring weekends of course)'''\n",
    "    business_days, df, api = pd.date_range(date-pd.DateOffset(weeks=1)+pd.DateOffset(days=1), date, freq='B'), [], br.paper_api() if not api else api\n",
    "    print(business_days)\n",
    "    for day in business_days:\n",
    "        print(day)\n",
    "        pdf = get_data(symbol, tk.get_market_open(day), tk.get_market_close(day), api=api)\n",
    "        df = pdf if not isinstance(df, pd.DataFrame) else easy_concat(df, pdf)\n",
    "    return df\n",
    "\n",
    "def construct_archive(symbol:str, weeks=1):\n",
    "    '''Constructs an archive for the given symbol using all available minute data that can be grabbed'''\n",
    "    df, api = None, br.paper_api()\n",
    "    if weeks > 0:\n",
    "        for i in range(weeks, -1, -1):\n",
    "            pdf = get_week(symbol, tk.today()-pd.DateOffset(weeks=i), api)\n",
    "            df = pdf if not isinstance(df, pd.DataFrame) else easy_concat(df, pdf)\n",
    "    else: df = get_data(symbol)\n",
    "    if not p.isdir('./archive_data/'+symbol.upper()): sub.run('mkdir ./archive_data/'+symbol.upper(), shell=True)\n",
    "    pd.to_pickle(df, './archive_data/'+symbol.upper()+'/main_archive.pkl')\n",
    "        \n",
    "def update_tracked_archives():\n",
    "    for symbol in tb.get_symbols('tracked_tickers.txt'):\n",
    "        construct_archive(symbol)\n",
    "\n",
    "def save_archive(symbol:str, name:str, data:pd.DataFrame = None):\n",
    "    '''Collects the data of a ticker and adds it to a pre-existing archive, sorting and making sure there are no duplicates'''\n",
    "    symbol_dir = './archive_data/'+symbol.upper()\n",
    "    archive_path = symbol_dir+'/'+name+'_main_archive.pkl'\n",
    "    if not p.isdir(symbol_dir): sub.run('mkdir '+symbol_dir,shell=True)\n",
    "    if not p.isfile(archive_path): data.to_pickle(archive_path); return True\n",
    "    archive_data = pd.read_pickle(archive_path)\n",
    "    joined = pd.concat([data, archive_data])\n",
    "    joined = joined[~joined.index.duplicated(keep='first')]\n",
    "    sub.run('cp '+archive_path+' '+archive_path+'.bckp', shell=True)\n",
    "    joined.to_pickle(archive_path)\n",
    "    if len(archive_data) != len(joined): return True\n",
    "    return False\n",
    "\n",
    "def get_archive(symbol:str):\n",
    "    '''Retrieves an archive of minute data from our stores and returns it as a normal datasource'''\n",
    "    symbol_dir = './archive_data/'+symbol.upper()\n",
    "    archive_path = symbol_dir+'/main_archive.pkl'\n",
    "    if not p.isfile(archive_path): archive_path = '.'+archive_path\n",
    "    archive_data = pd.read_pickle(archive_path)\n",
    "    archive_data = archive_data[~archive_data.index.duplicated(keep='last')]\n",
    "    archive_data.columns = archive_data.columns.str.lower()\n",
    "    return archive_data\n",
    "\n",
    "def easy_concat(df1:pd.DataFrame, df2:pd.DataFrame):\n",
    "    '''Quick and dirty wrapper method to concat two dataframes and remove duplicates'''\n",
    "    df = pd.concat([df1,df2])\n",
    "    df = df[~df.index.duplicated(keep='last')]\n",
    "    return df.sort_index()\n",
    "\n",
    "def get_segment(symbol:str, api:alp.REST):\n",
    "    '''Get's the last minute of the symbol and waits if it doesn't match'''\n",
    "    tick = tk.now().round(f'{tk.TICK_DURATION}s')\n",
    "    bar = get_trades(symbol, tick-pd.DateOffset(seconds=tk.TICK_DURATION), tick).price\n",
    "    bar = bar[(np.abs(stats.zscore(bar)) < 1)]\n",
    "    bar = pd.DataFrame([{'date':tick, 'open': bar.iloc[0], 'close': bar.iloc[-1], 'high':bar.max(), 'low':bar.min(), 'volume':len(bar)}]).set_index('date').iloc[-1]\n",
    "    return bar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping for 3\n",
      "2024-05-02 12:27:21.055577832-04:00\n",
      "2024-05-02 12:27:13-04:00\n",
      "open      847.6724\n",
      "close     847.3900\n",
      "high      847.7300\n",
      "low       847.3900\n",
      "volume    140.0000\n",
      "Name: 2024-05-02 12:27:10-04:00, dtype: float64\n",
      "sleeping for 8\n",
      "2024-05-02 12:27:21-04:00\n",
      "open      847.300\n",
      "close     847.440\n",
      "high      847.455\n",
      "low       847.275\n",
      "volume     68.000\n",
      "Name: 2024-05-02 12:27:20-04:00, dtype: float64\n",
      "sleeping for 10\n",
      "2024-05-02 12:27:31-04:00\n",
      "open      847.4324\n",
      "close     847.4000\n",
      "high      847.5200\n",
      "low       847.3000\n",
      "volume     60.0000\n",
      "Name: 2024-05-02 12:27:30-04:00, dtype: float64\n",
      "sleeping for 10\n",
      "2024-05-02 12:27:41-04:00\n",
      "open      847.400\n",
      "close     847.480\n",
      "high      847.485\n",
      "low       847.120\n",
      "volume    104.000\n",
      "Name: 2024-05-02 12:27:40-04:00, dtype: float64\n",
      "sleeping for 10\n",
      "2024-05-02 12:27:51-04:00\n",
      "open      847.390\n",
      "close     847.265\n",
      "high      847.420\n",
      "low       847.160\n",
      "volume     61.000\n",
      "Name: 2024-05-02 12:27:50-04:00, dtype: float64\n",
      "sleeping for 10\n",
      "2024-05-02 12:28:01-04:00\n",
      "open      847.4253\n",
      "close     847.2500\n",
      "high      847.4253\n",
      "low       847.2437\n",
      "volume     66.0000\n",
      "Name: 2024-05-02 12:28:00-04:00, dtype: float64\n",
      "sleeping for 9\n",
      "2024-05-02 12:28:11-04:00\n",
      "open      847.340\n",
      "close     847.335\n",
      "high      847.460\n",
      "low       847.250\n",
      "volume     44.000\n",
      "Name: 2024-05-02 12:28:10-04:00, dtype: float64\n",
      "sleeping for 10\n",
      "2024-05-02 12:28:21-04:00\n",
      "open      847.500\n",
      "close     847.510\n",
      "high      847.620\n",
      "low       847.377\n",
      "volume     56.000\n",
      "Name: 2024-05-02 12:28:20-04:00, dtype: float64\n",
      "sleeping for 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m bar \u001b[38;5;241m=\u001b[39m get_segment(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNVDA\u001b[39m\u001b[38;5;124m'\u001b[39m, api)\n\u001b[1;32m     14\u001b[0m data\u001b[38;5;241m.\u001b[39mloc[bar\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m bar\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/algotrading4/notebooks/../lib/TimeKeeper.py:166\u001b[0m, in \u001b[0;36msync\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m sleep_time \u001b[38;5;241m=\u001b[39m (TICK_DURATION \u001b[38;5;241m-\u001b[39m now()\u001b[38;5;241m.\u001b[39msecond \u001b[38;5;241m%\u001b[39m TICK_DURATION) \u001b[38;5;241m+\u001b[39m TICK_OFFSET\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msleeping for\u001b[39m\u001b[38;5;124m'\u001b[39m, sleep_time)\n\u001b[0;32m--> 166\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_time\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "'''The actual cycle of keeping a vigilent eye on the market and seeing if we should trade'''\n",
    "open_date = tk.get_trade_open()\n",
    "if tk.is_after(open_date.hour, open_date.minute): tk.sync()\n",
    "#self.post_to_slack()\n",
    "data = update_archive('NVDA')\n",
    "while True:\n",
    "    # if tk.keep_time(self.port):\n",
    "    #     self.update_tickers()\n",
    "    #     tk.sync()\n",
    "    #     self.post_to_slack()\n",
    "    print(tk.now())\n",
    "    # self.trade(tk.now())\n",
    "    bar = get_segment('NVDA', api)\n",
    "    data.loc[bar.name] = bar\n",
    "    tk.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_logs(symbol:str, date:pd.DatetimeIndex = tk.today()):\n",
    "    entry_name = date.strftime(\"%Y-%m-%d-%a\")\n",
    "    brokerage_p, training_p = p.join('logs','brokerage',entry_name,symbol.upper()+'_BR_OPS.tsv'),  p.join('logs','training',entry_name,symbol.upper()+'_TR_OPS.tsv')\n",
    "    if not p.isfile(brokerage_p): brokerage_p = '../'+brokerage_p\n",
    "    if not p.isfile(training_p): training_p = '../'+training_p\n",
    "    broker, training = pd.read_csv(brokerage_p, delimiter='\\t').set_index('index'), pd.read_csv(training_p, delimiter='\\t').set_index('index')\n",
    "    for index in broker.index:\n",
    "        if index in training.index:\n",
    "            be, te = broker.loc[index], training.loc[index]\n",
    "            if be.close==te.close and be.bollinger_high and te.bollinger_high and be.volume == te.volume:\n",
    "                continue\n",
    "            print(index, 'DISCREPENCY')\n",
    "\n",
    "\n",
    "compare_logs('NVDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nEvery 10 seconds we pull data\\nEvery 60 seconds we check it against our archive (after updating of course)\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Every 10 seconds we pull data\n",
    "Every 60 seconds we check it against our archive (after updating of course)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
